"""
Servi√ßo de integra√ß√£o com Google Gemini para o Chatbot Liter√°rio.
Gerencia conversas, contexto e respostas da IA.
"""
import logging
from typing import Optional, Dict, List
from django.conf import settings
import google.generativeai as genai

logger = logging.getLogger(__name__)


class GeminiChatbotService:
    """
    Servi√ßo para gerenciar conversas com o Google Gemini AI.

    Caracter√≠sticas:
    - Foco em literatura, livros e cultura da leitura
    - Respostas objetivas mas com emo√ß√£o
    - Manuten√ß√£o de contexto da conversa
    - Tratamento de assuntos fora do escopo
    """

    # Prompt do sistema - Define a personalidade e escopo do chatbot
    SYSTEM_PROMPT = """Voc√™ √© o Assistente Liter√°rio da CG.BookStore.

REGRAS ABSOLUTAS (SIGA RIGOROSAMENTE):

1. Use o nome do usu√°rio APENAS na primeira sauda√ß√£o ou quando fizer sentido natural no contexto
2. CG.BookStore √© COMUNIDADE/APLICA√á√ÉO WEB - N√ÉO vendemos livros
3. Indique Amazon como parceiro para compras
4. Seja CONCISO - m√°ximo 2-3 frases por t√≥pico
5. Sempre recomende 3 T√çTULOS ESPEC√çFICOS, nunca categorias gen√©ricas
6. Usu√°rio est√° DENTRO da aplica√ß√£o - busca √© "lupa ali em cima"
7. Nosso "cat√°logo" = banco de DADOS de informa√ß√µes (n√£o vendas)

‚ö†Ô∏è REGRA CR√çTICA - ANTI-ALUCINA√á√ÉO:
8. NUNCA invente informa√ß√µes sobre livros, autores ou datas de publica√ß√£o
9. Se voc√™ N√ÉO tiver CERTEZA ABSOLUTA sobre um livro, diga:
   "N√£o encontrei essa informa√ß√£o no nosso banco de dados. Quer que eu ajude a buscar?"
10. Quando em D√öVIDA, sempre escolha: "N√£o tenho certeza" em vez de chutar
11. NUNCA force uma resposta se n√£o souber - seja HONESTO

O QUE √â CG.BOOKSTORE:
- Comunidade de leitores
- Organiza√ß√£o de estantes pessoais (Quero Ler, Lendo, Lidos)
- Banco de dados com informa√ß√µes sobre livros
- Entrevistas, v√≠deos, eventos liter√°rios
- M√©dia de pre√ßos do mercado
- Indica√ß√£o de parceiros (Amazon)

VOCABUL√ÅRIO PROIBIDO:
‚ùå "vendemos livros", "nosso estoque", "dispon√≠vel aqui", "acesse o site"

VOCABUL√ÅRIO CORRETO:
‚úÖ "indicamos Amazon", "banco de dados", "lupa ali em cima", "voc√™ est√° na aplica√ß√£o"

EXEMPLO DE RESPOSTA:
Usu√°rio: "Me recomende fic√ß√£o cient√≠fica"
Voc√™: "Aqui v√£o 3 t√≠tulos excelentes:
1. **Neuromancer** (Gibson) - Cyberpunk cl√°ssico
2. **Problema dos Tr√™s Corpos** (Cixin) - Sci-fi hard
3. **M√£o Esquerda da Escurid√£o** (Le Guin) - Quest√µes sociais
Qual te interessa mais?"

ONDE COMPRAR:
"CG.BookStore √© comunidade, n√£o vendemos. Indicamos **Amazon**:
üì¶ Onde: Amazon
üí∞ M√©dia: R$ XX-XX*
*Valores aproximados"

EXEMPLO - QUANDO N√ÉO SOUBER:
Usu√°rio: "Quem escreveu Quarta Asa?"
Voc√™ (se n√£o tiver certeza): "N√£o encontrei 'Quarta Asa' no nosso banco de dados. Pode verificar se o t√≠tulo est√° correto? Posso ajudar a buscar usando a lupa ali em cima."

ESCOPO:
‚úÖ Literatura, livros, autores, g√™neros, recomenda√ß√µes
‚úÖ Adapta√ß√µes (filmes, s√©ries, anime, games, quadrinhos)
‚úÖ Tecnologia liter√°ria (e-books, audiobooks)
‚úÖ Funcionalidades da plataforma

‚ùå Assuntos fora de literatura: redirecione gentilmente"""

    def __init__(self):
        """Inicializa o servi√ßo do chatbot."""
        self.api_key = settings.GEMINI_API_KEY
        self.model_name = 'gemini-2.0-flash-exp'  # Modelo mais recente e r√°pido
        self._model = None

        # Configura√ß√µes de seguran√ßa mais permissivas para conte√∫do liter√°rio
        self.safety_settings = [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_NONE"  # Livros podem conter debates intensos
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "threshold": "BLOCK_ONLY_HIGH"  # An√°lise liter√°ria pode discutir temas sens√≠veis
            },
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "threshold": "BLOCK_ONLY_HIGH"  # Literatura pode conter romance/intimidade
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "threshold": "BLOCK_ONLY_HIGH"  # Livros de suspense/terror existem
            },
        ]

        # Configura√ß√µes de gera√ß√£o
        self.generation_config = {
            "temperature": 0.3,  # Baixa temperatura = mais obediente √†s regras
            "top_p": 0.8,  # Reduzido para respostas mais focadas
            "top_k": 20,  # Reduzido para maior consist√™ncia
            "max_output_tokens": 1024,  # Reduzido para for√ßar concis√£o
        }

        logger.info("gemini_service", "Inicializando servi√ßo do chatbot liter√°rio...")

    @property
    def model(self):
        """Lazy loading do modelo Gemini."""
        if self._model is None:
            if not self.api_key:
                raise ValueError("GEMINI_API_KEY n√£o configurada nas vari√°veis de ambiente")

            logger.info("gemini_service", "google.generativeai loaded successfully for chatbot")
            genai.configure(api_key=self.api_key)

            logger.info("gemini_service", f"Inicializando modelo Gemini para chatbot ({self.model_name})...")
            self._model = genai.GenerativeModel(
                model_name=self.model_name,
                generation_config=self.generation_config,
                safety_settings=self.safety_settings,
                system_instruction=self.SYSTEM_PROMPT
            )
            logger.info("gemini_service", "Modelo Gemini para chatbot inicializado com sucesso")

        return self._model

    def is_available(self) -> bool:
        """Verifica se o servi√ßo est√° dispon√≠vel."""
        try:
            _ = self.model
            return True
        except Exception as e:
            logger.error("gemini_service", f"Servi√ßo indispon√≠vel: {e}")
            return False

    def get_response(
        self,
        message: str,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> str:
        """
        Gera uma resposta do chatbot para a mensagem do usu√°rio.

        Args:
            message: Mensagem do usu√°rio
            conversation_history: Lista de mensagens anteriores no formato:
                [{"role": "user", "parts": ["mensagem"]}, {"role": "model", "parts": ["resposta"]}]

        Returns:
            Resposta do chatbot

        Raises:
            Exception: Se houver erro na comunica√ß√£o com a API
        """
        try:
            logger.info("gemini_service", f"Enviando mensagem ao Gemini: {message[:100]}...")

            # Criar sess√£o de chat com hist√≥rico
            chat = self.model.start_chat(history=conversation_history or [])

            # Enviar mensagem
            response = chat.send_message(message)

            # Verificar finish_reason
            finish_reason = response.candidates[0].finish_reason
            logger.info("gemini_service", f"Finish reason: {finish_reason}")

            # finish_reason pode ser:
            # 0 = FINISH_REASON_UNSPECIFIED
            # 1 = STOP (resposta completa - OK)
            # 2 = MAX_TOKENS (atingiu limite de tokens)
            # 3 = SAFETY (bloqueado por seguran√ßa)
            # 4 = RECITATION (bloqueado por recita√ß√£o/pl√°gio)
            # 5 = OTHER

            if finish_reason == 1:  # STOP - resposta completa
                bot_response = response.text.strip()
                logger.info("gemini_service", f"Resposta recebida com sucesso ({len(bot_response)} chars)")
                return bot_response

            elif finish_reason == 2:  # MAX_TOKENS
                logger.warning("gemini_service", "Resposta atingiu limite de tokens")
                if response.text:
                    return response.text.strip() + "\n\n[Resposta foi cortada por limite de tamanho. Pe√ßa para continuar!]"
                else:
                    return "Desculpe, a resposta ficou muito longa. Pode reformular a pergunta de forma mais espec√≠fica? üìö"

            elif finish_reason == 3:  # SAFETY
                logger.warning("gemini_service", "Resposta bloqueada por filtros de seguran√ßa")
                safety_ratings = response.candidates[0].safety_ratings
                logger.warning("gemini_service", f"Safety ratings: {safety_ratings}")

                return ("Ops! Parece que sua pergunta acionou os filtros de seguran√ßa. üîí "
                        "Vamos manter nossa conversa focada em literatura e livros? "
                        "Posso te ajudar com recomenda√ß√µes, an√°lises liter√°rias ou d√∫vidas sobre o CG.BookStore! üìö‚ú®")

            elif finish_reason == 4:  # RECITATION
                logger.warning("gemini_service", "Resposta bloqueada por recita√ß√£o")
                return ("Essa resposta cont√©m muito conte√∫do de fontes existentes. "
                        "Posso reformular ou dar minha pr√≥pria perspectiva sobre o assunto? üìñ")

            else:  # UNSPECIFIED ou OTHER
                logger.error("gemini_service", f"Finish reason inesperado: {finish_reason}")
                if response.text:
                    return response.text.strip()
                else:
                    return ("Hmm, algo inesperado aconteceu. ü§î "
                            "Pode tentar perguntar de outra forma? Estou aqui para ajudar! üí¨")

        except Exception as e:
            logger.error("gemini_service", f"Erro ao gerar resposta do chatbot: {e}")
            raise Exception(f"Erro ao processar mensagem: {e}")

    def format_history_for_gemini(
        self,
        messages: List[Dict[str, str]]
    ) -> List[Dict[str, any]]:
        """
        Formata hist√≥rico de mensagens para o formato esperado pelo Gemini.

        Args:
            messages: Lista de mensagens no formato:
                [{"role": "user", "content": "msg"}, {"role": "assistant", "content": "resp"}]

        Returns:
            Lista no formato do Gemini:
                [{"role": "user", "parts": ["msg"]}, {"role": "model", "parts": ["resp"]}]
        """
        formatted = []
        for msg in messages:
            role = "model" if msg["role"] == "assistant" else msg["role"]
            formatted.append({
                "role": role,
                "parts": [msg["content"]]
            })
        return formatted


# Inst√¢ncia singleton do servi√ßo
_chatbot_service = None


def get_gemini_service() -> GeminiChatbotService:
    """Retorna a inst√¢ncia singleton do servi√ßo de chatbot Gemini."""
    global _chatbot_service
    if _chatbot_service is None:
        _chatbot_service = GeminiChatbotService()
    return _chatbot_service


def get_chatbot_service():
    """
    Retorna o servi√ßo de chatbot configurado (Gemini ou Groq).

    Escolhe automaticamente baseado na vari√°vel AI_PROVIDER no .env:
    - 'gemini': Usa Google Gemini (padr√£o se n√£o especificado)
    - 'groq': Usa Groq AI (recomendado - mais r√°pido e free tier generoso)

    Returns:
        Inst√¢ncia do servi√ßo de chatbot (GeminiChatbotService ou GroqChatbotService)
    """
    ai_provider = getattr(settings, 'AI_PROVIDER', 'gemini').lower()

    logger.info(f"Usando provedor de IA: {ai_provider}")

    if ai_provider == 'groq':
        try:
            from .groq_service import get_groq_chatbot_service
            service = get_groq_chatbot_service()
            logger.info("‚úÖ Servi√ßo Groq inicializado com sucesso")
            return service
        except Exception as e:
            logger.error(f"‚ùå Erro ao inicializar Groq: {e}")
            logger.info("‚ö†Ô∏è Fallback para Gemini")
            return get_gemini_service()
    else:
        # Padr√£o: Gemini
        return get_gemini_service()
